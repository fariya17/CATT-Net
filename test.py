# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YV2fwJ-5ydI5zmY_ojdelrhJYnUgR-8M
"""

import numpy as np
import pandas as pd
import argparse
import pickle
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from tensorflow.keras.models import load_model

def load_preprocessors(preprocessor_path):
    """
    Load preprocessors from a pickle file

    Parameters:
    -----------
    preprocessor_path : str
        Path to the preprocessor pickle file

    Returns:
    --------
    preprocessors : dict
        Dictionary of preprocessors
    """
    with open(preprocessor_path, 'rb') as f:
        return pickle.load(f)

def squeeze_and_excitation(features, ratio=16):
    """
    Apply Squeeze-and-Excitation block to recalibrate feature maps

    Parameters:
    -----------
    features : numpy array
        Input features (e.g., from PCA)
    ratio : int
        Reduction ratio for the bottleneck

    Returns:
    --------
    scaled : numpy array
        Recalibrated features
    """
    # Reshape for SE block processing (assuming features from PCA)
    se_input = features.reshape(features.shape[0], 1, features.shape[1])

    # Squeeze operation (global average pooling)
    squeeze = np.mean(se_input, axis=1, keepdims=True)

    # Excitation operation (two dense layers with bottleneck)
    reduced_dim = max(1, se_input.shape[2] // ratio)

    # First dense layer with ReLU (manually)
    w1 = np.random.normal(size=(se_input.shape[2], reduced_dim))
    excitation = np.maximum(0, np.dot(squeeze, w1))  # ReLU activation

    # Second dense layer with Sigmoid (manually)
    w2 = np.random.normal(size=(reduced_dim, se_input.shape[2]))
    excitation = 1 / (1 + np.exp(-np.dot(excitation, w2)))  # Sigmoid activation

    # Scale the original features
    scaled = se_input * excitation

    return scaled.reshape(features.shape[0], features.shape[1])

def preprocess_data(X, preprocessors):
    """
    Preprocess data for prediction

    Parameters:
    -----------
    X : array-like
        Feature matrix
    preprocessors : dict
        Dictionary of preprocessors

    Returns:
    --------
    X_processed : numpy array
        Processed data ready for model input
    """
    # Scale the features
    X_scaled = preprocessors['scaler'].transform(X)

    # Apply PCA
    X_pca = preprocessors['pca'].transform(X_scaled)

    # Apply Squeeze-and-Excitation
    X_se = squeeze_and_excitation(X_pca)

    # Reshape for LSTM
    X_lstm = X_se[..., np.newaxis]

    return X_lstm

def predict(model, X_processed, preprocessors):
    """
    Make predictions with the trained model

    Parameters:
    -----------
    model : Keras Model
        Trained model
    X_processed : numpy array
        Processed input data
    preprocessors : dict
        Dictionary of preprocessors

    Returns:
    --------
    y_pred : array
        Predicted class labels
    y_pred_proba : array
        Predicted class probabilities
    """
    # Get predicted probabilities
    y_pred_proba = model.predict(X_processed)

    # Convert to class indices
    y_pred_indices = np.argmax(y_pred_proba, axis=1)

    # Convert back to original labels
    y_pred = preprocessors['label_encoder'].inverse_transform(y_pred_indices)

    return y_pred, y_pred_proba

def evaluate_model(y_true, y_pred, class_names=None):
    """
    Evaluate model performance

    Parameters:
    -----------
    y_true : array-like
        True class labels
    y_pred : array-like
        Predicted class labels
    class_names : list, optional
        List of class names

    Returns:
    --------
    metrics : dict
        Dictionary of evaluation metrics
    """
    # Calculate accuracy
    accuracy = accuracy_score(y_true, y_pred)

    # Generate classification report
    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)

    # Calculate confusion matrix
    cm = confusion_matrix(y_true, y_pred)

    return {
        'accuracy': accuracy,
        'classification_report': report,
        'confusion_matrix': cm
    }

def plot_confusion_matrix(cm, class_names=None, save_path=None):
    """
    Plot confusion matrix

    Parameters:
    -----------
    cm : array-like
        Confusion matrix
    class_names : list, optional
        List of class names
    save_path : str, optional
        Path to save the figure

    Returns:
    --------
    fig : matplotlib figure
        Figure with confusion matrix plot
    """
    fig, ax = plt.subplots(figsize=(10, 8))

    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        cmap='Blues',
        ax=ax,
        xticklabels=class_names if class_names else "auto",
        yticklabels=class_names if class_names else "auto"
    )

    ax.set_title('Confusion Matrix')
    ax.set_ylabel('True Label')
    ax.set_xlabel('Predicted Label')

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path)

    return fig

def parse_args():
    parser = argparse.ArgumentParser(description='Test the hybrid LSTM-Transformer model')
    parser.add_argument('--data', type=str, required=True, help='Path to the test dataset CSV file')
    parser.add_argument('--target', type=str, required=True, help='Name of the target column')
    parser.add_argument('--model', type=str, required=True, help='Path to the trained model')
    parser.add_argument('--preprocessors', type=str, required=True, help='Path to the preprocessors pickle file')
    parser.add_argument('--output-dir', type=str, default='output', help='Output directory')

    return parser.parse_args()

def main():
    # Parse command-line arguments
    args = parse_args()

    # Create output directory if it doesn't exist
    os.makedirs(args.output_dir, exist_ok=True)

    # Load test dataset
    print(f"Loading test dataset from {args.data}...")
    dataset = pd.read_csv(args.data)

    # Split features and target
    if args.target not in dataset.columns:
        raise ValueError(f"Target column '{args.target}' not found in dataset")

    X_test = dataset.drop(columns=[args.target])
    y_test = dataset[args.target]

    print(f"Test dataset shape: {X_test.shape}")

    # Load model and preprocessors
    print(f"Loading model from {args.model}...")
    model = load_model(args.model)

    print(f"Loading preprocessors from {args.preprocessors}...")
    preprocessors = load_preprocessors(args.preprocessors)

    # Preprocess test data
    print("Preprocessing test data...")
    X_test_processed = preprocess_data(X_test, preprocessors)

    # Make predictions
    print("Making predictions...")
    y_pred, y_pred_proba = predict(model, X_test_processed, preprocessors)

    # Evaluate model performance
    print("Evaluating model performance...")
    class_names = preprocessors['label_encoder'].classes_
    metrics = evaluate_model(y_test, y_pred, class_names=class_names)

    # Print performance metrics
    print(f"\nTest Accuracy: {metrics['accuracy'] * 100:.2f}%")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=class_names))

    # Plot and save confusion matrix
    cm_path = os.path.join(args.output_dir, 'confusion_matrix.png')
    print(f"Saving confusion matrix to {cm_path}...")
    plot_confusion_matrix(metrics['confusion_matrix'], class_names=class_names, save_path=cm_path)

    # Save detailed results to CSV
    results_df = pd.DataFrame({
        'True': y_test.values,
        'Predicted': y_pred
    })

    results_path = os.path.join(args.output_dir, 'prediction_results.csv')
    results_df.to_csv(results_path, index=False)
    print(f"Saved prediction results to {results_path}")

    # Save metrics to JSON
    metrics_df = pd.DataFrame({
        'Metric': ['Accuracy'] + [f"{cls}_precision" for cls in class_names] + [f"{cls}_recall" for cls in class_names] + [f"{cls}_f1-score" for cls in class_names],
        'Value': [metrics['accuracy']] +
                [metrics['classification_report'][cls]['precision'] for cls in class_names] +
                [metrics['classification_report'][cls]['recall'] for cls in class_names] +
                [metrics['classification_report'][cls]['f1-score'] for cls in class_names]
    })

    metrics_path = os.path.join(args.output_dir, 'metrics.csv')
    metrics_df.to_csv(metrics_path, index=False)
    print(f"Saved metrics to {metrics_path}")

    print("\nTesting completed successfully!")

if __name__ == "__main__":
    main()